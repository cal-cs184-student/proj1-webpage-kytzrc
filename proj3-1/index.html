<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
  p.big {line-height:150%;}
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">RUICHI ZHANG & YUNTIAN KE</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="TODO">TODO</a></h2>

<br><br>


<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/task5/spheres_adapt.png" width="480px" />
          <figcaption align="middle">Results Caption: my shperes are the bounciest shperes</figcaption>
      </tr>
  </table>
</div>


<h2 align="middle">Overview</h2>
<p class="big">
    In this project, we implement the core routines of a physically-based renderer using a pathtracing algorithm. We generate sample rays from a camera and caculate the integral of the 
    global illumination of the samples. To trace the rays, we implement ray-triangle and ray-shpere intersection algorithms to find intersections between rays and the scene. In
    particular, we use Moller Trumbore algorithm to optimize ray-triangle intersection, which minish precision error. We then implement
    BVH structure to accelerate pathtracing, taking ray intersection complexity from \(O(n)\) to \(O(log(n))\). Then, in part3 and part4, we implement illumination algorithms to simulating
    light transport. Specifically, we implement direct illumination first (including hemisphere and importance sampling) and then implement global illumination using Russian Roulette. 
    Finally, we implement adaptive sampling to reduce the noise by concentrating the samples in the more difficult parts of the image, i.e. samples that converge slowly.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p class="big">
    To render a picture, the renderer first generate sample rays from the camera. For every pixel in the image, we map it to the sensor to generate rays in camera space
    and then transform the generated rays to world space. Then, we calculate illumination of each ray by PathTracer::est_radiance_global_illumination() and average the integral.
    For primitives intersection part, we implement ray-triangle intersection and ray-sphere intersection.
    For ray-triangle intersection, we first intersect the ray and the triangle plane to get the hit point. Then, we check if the hit point lies inside the triangle according to its 
    barycentric coordinates. For ray-sphere intersection, we calculate the hit points by solving the line-shpere equation, which is quadratic. If the ray hits a primitive, we need
    to store the information of the hit point in a Intersection *isect structure as well.
</p>
<br>

<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p class="big">
    We have the origin <b>O</b> and direction <b>D</b> of the ray, as well as three vertices of the triangle <b>P<sub>0</sub></b>,<b>P<sub>1</sub></b>,<b>P<sub>2</sub></b>
    .Then, we could use Moller Trumbore algorithm (shown as below) to calculate the hit point (\(t\)) and barycentric coordinates (<i>b<sub>1</sub></i>, <i>b<sub>2</sub></i>) all at once. Then, 
    we calculate <i>b<sub>0</sub></i>, which equals to 1-<i>b<sub>1</sub></i>-<i>b<sub>2</sub></i>. If <i>b<sub>0</sub></i>,<i>b<sub>1</sub></i>,<i>b<sub>2</sub></i> are all between 0
    and 1, the hit point is in the triangle, which means there is an intersection. Otherwise, there is no intersection. Note that there is a valid intersection only if 
    \(t\) lies within min_t and max_t of the ray, and we have to update max_t to t accordingly. For Triangle::intersect(), we also need to store the information of the hit point 
    in Intersection *isect structure for later use. 

</p>
<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/mollertrumber.png" width="480px" />
          <figcaption align="middle">Moller Trumbore algorithm</figcaption>
      </tr>
  </table>
</div>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task1/cube.png" align="middle" width="400px"/>
        <figcaption>cube.dae</figcaption>
      </td>
      <td>
        <img src="images/task1/CBempty.png" align="middle" width="400px"/>
        <figcaption>CBempty.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task1/CBspheres.png" align="middle" width="400px"/>
        <figcaption>CBspheres.dae</figcaption>
      </td>
      <td>
        <img src="images/task1/banana.png" align="middle" width="400px"/>
        <figcaption>banana.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p class="big">
    We construct the BVH tree recursively. We use a vector to store all the primitives. 
    The node is a leaf node when it contains less than max_leaf_size primitives and we store the start and end iterators of its primitive list the node structure.
    Otherwise, we divide the primitives into two halves and construct its left and right children, each half of primitives belongs to its corresponding children. We chose
    the average of centroids to be the splitting point and we chose to split along the axis which produces least intersection cost. The estimated cost is SA(L)*TriCount(L)+SA(R)*TriCount(R),
    where SA denotes surface area of the bounding box and TriCount denotes number of primitives in the box.
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task2/maxplanck.png" align="middle" width="400px"/>
        <figcaption>maxplank.dae</figcaption>
      </td>
      <td>
        <img src="images/task2/peter.png" align="middle" width="400px"/>
        <figcaption>peter.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task2/CBlucy.png" align="middle" width="400px"/>
        <figcaption>CBlucy.dae</figcaption>
      </td>
      <td>
        <img src="images/task2/CBdragon.png" align="middle" width="400px"/>
        <figcaption>CBdragon.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p class="big">
    Without BVH acceleration, rendering maxplank.dae takes 195s and peter.dae takes 129s, which is quite a long time since these
    two files both contain tens of thousands triangles. On the other hand, with BVH acceleration, we can render these two image in 0.05s and 0.04s repectively. 
    This is because BVH structure lowers the time complexity from \(O(n)\) to \(O(log(n))\).
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p class="big">
    Uniform hemisphere sampling sample imcoming directions uniformly from a hemisphere and use Monte Carlo estimator to calculate the total illumination.
    If the sample light intersect a light source, we add the direct light to the point, otherwise, we do nothing. For the implementation, we first calculate the 
    hit point and outgoing direction. Then, we sample object directions from uniform distribution with pdf = 1/(2*PI) and calculate cos(theta). With incoming and ougoing direction,
    we can get the bsdf value of the hit point. Then, we check if the ray intersect with any objects in the scene. If it does, we use bsdf->get_emission() to get the light from that direction.
    Finally, we could compute the estimator as the equation shown below.

</p>

<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/estimator.png" width="480px" />
          <figcaption align="middle">Monte Carlo estimator</figcaption>
      </tr>
  </table>
</div>
<br>

<p class="big">
    For importance sampling lights, we want to sample just from light sources, so we iterate every light sources and sample from these light direction.
    For a single light point, we just need sample once, because every time we sample we will get the same result. Otherwise, we still sample multiple times from the light direction.
    Note that we don't have to sample if the light source is below the hit surface, since it will never reach the hit point We could use SceneLight::sample_L() to get the pdf and incoming
    direction. Then, we could plug into the Monte Carlo estimator as before.
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/task3/CBbunny_H_64_32.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="images/task3/bunny_64_32.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/task3/CBdragon_H_64_32.png" align="middle" width="400px"/>
        <figcaption>CBdragon.dae</figcaption>
      </td>
      <td>
        <img src="images/task3/dragon_64_32.png" align="middle" width="400px"/>
        <figcaption>CBdragon.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task3/spheres1.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/task3/spheres4.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task3/spheres16.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/task3/spheres64.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p class="big">
    With light rays number increases, the noise in soft shadows is reduced.  
    The shadow is quite noisy with 1 light ray, while more smooth with 64 light rays.
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p class="big">
    As we can see, uniform sampling is much more noisy. That is because we simply sample from all the directions and it is possible that all the incoming ray samples are not from light direction.
    If so, the point will have no irradiance and becomes black. To reduce such noise, we want to sample directly from the light source, which could ensure every point 
    that can be reached by the light have at least some irradiance. Importance light sampling also enable us to render images that only have point lights.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p class="big">
    We implement indirect lighting in at_least_one_bounce_radiance() function. First, we calculate the direct illumination by calling one_bounce_radiance() function.
    Then, we generate a sample incoming path by calling BSDF::sample_f(), which becomes the outgoing light when we recursively call the at_least_one_bounce_radiance() function. By doing so, we create a path 
    that bouncing in the scene and we then calcuulate the indirect illumination by summing up all the illumination transmitted through the path. To end the infinite recursion, we use 
    Russian Roulette algorithm, which in every step we continue with a probability. We also need to normalize the estimator to keep it unbiased. Besides, we also assign each ray a depth, denoting
    the time that a ray bounce. The recursion is terminated when the bounce time of a ray reaches max_ray_depth.
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task4/bunny1024.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="images/task4/spheres1024.png" align="middle" width="400px"/>
        <figcaption>CBspheres.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task4/spheres1024-dir.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (CBspheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/task4/spheres1024-indir.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (CBspheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p class="big">
    With only direct illumination, the whole scene looks darker, especially for places like ceiling that doesn't receive direct light, which is completely black. With only indirect
    illumination, the color of the scene is brighter and more moderate. However, the lack of direct light makes the scene look dim.
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task4/bunny0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/task4/bunny1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task4/bunny2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/task4/bunny3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task4/bunny100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p class="big">
    0 max depth refers to zero bounce illumination, which only include the light source. 1 max depth refers to the direct illumination, the scene is dark since there are only direct light.
    when max depth is greater than 1, indirect light is introduced and we can see in the rendered images that with the increase of the max depth, the color becomes brighter and the scene looks lighter.
    Note that the larger the max depth is, the more likely that the recursion will be terminated by Russian Roulette. Thus, when max depth is 100, which is far greater than 2 or 3, the scene actually doesn't very much.
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task4/spheres01.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/task4/spheres02.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task4/spheres04.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/task4/spheres08.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task4/spheres016.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/task4/spheres064.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task4/spheres01024.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    The scene becomes less noisy when we increase the sample-per-pixel rate.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p class="big">
    Adaptive sampling is a method that focus on pixels that needs more samples to get rid of noise. It tries to avoid using a fixed high number of samples per pixel by concentrating the 
    samples in the more difficult parts of the image. For the implementation, we calculate a variable $I = 1.96\cdot\frac{\sigma}{\sqrt{n}}$ and we conclude that the pixel has converged
    when $I$ is small enough. Specifically, every samplesPerBatch pixels we check if $I \le maxTolerance\cdot\mu$. If a pixel has converged, we stop sampling and move to the next pixel. 
    We could keep $s_1 = \sum_{k=1}^{n}x_k$ and $s_2 = \sum_{k=1}^{n}x_k^2$. Then, the mean and variance can be expressed as $\mu = \frac{s_1}{n}$ and $\sigma^2 = \frac{1}{n-1}\cdot\left(s_2-\frac{s_1^2}{n}\right)$.
    Then, we could easily check if a pixel is converged by calculating $I$ using mean and variance we get.
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/task5/spheres_adapt.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/task5/spheres_adapt_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/task5/bunny.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/task5/bunny_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p class="big">
  We can see that in both images we put less concentration on the background and focus more on the main object (bunny and spheres).
</p>

</body>
</html>
